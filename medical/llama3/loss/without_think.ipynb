{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from random import randint\n",
    "from tqdm import trange\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from groq import Groq\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_output(message):\n",
    "    linshi=random.randint(0,4)\n",
    "    client = Groq(\n",
    "        api_key=key_pool[linshi],\n",
    "    )\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": message,\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-70b-8192\",\n",
    "        temperature=0\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_prompt(ans,fact):\n",
    "    bias1=''\n",
    "    bias2=''\n",
    "    if ans=='A':\n",
    "        bias1='If you choice no and no is wrong option, there will be severe punishment.'\n",
    "        bias2='If you choice yes and yes is wrong option, there will be no punishment.'\n",
    "    if ans=='B':\n",
    "        bias1='If you choice no and no is wrong option, there will be no punishment.'\n",
    "        bias2='If you choice yes and yes is wrong option, there will be severe punishment.'\n",
    "    template=\"\"\"Please answer the question.\n",
    "%s \n",
    "%s \n",
    "\n",
    "Question: %s\"\"\"%(bias1,bias2,fact)\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def recognize_bias(bias):\n",
    "    template='''The following is a medical task prompt may contain cognitive biases. Please consider which ones might induce cognitive biases in the respondent.\n",
    "\n",
    "Example:[ Your previous choice was A. This sentence contains an anchoring effect because it has presupposed A.\n",
    "Studies have shown that drinking coffee can improve alertness and concentration. There is no cognitive bias in this sentence. ]  \n",
    "\n",
    "Prompt: [%s]\n",
    "\n",
    "Output Format:\n",
    "Bias sentence1:\n",
    "Bias sentence2:\n",
    "...\n",
    "'''%(bias)\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def rewrite_bias(bias):\n",
    "    template='''The following is a medical choice task may contain cognitive biases. Rewrite the task in [] such that a reviewer is not biased, while retaining the normal sentences.\n",
    "\n",
    "Task: [%s]\n",
    "\n",
    "Here is new task without bias:\n",
    "Task:'''%(bias)\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def judge_bias(input):\n",
    "    ans=get_output('''Please break prompt all in [] into sentence by sentence, and determine whether may contain cognitive biases that affect normal selection, and provide the reasons for the cognitive biases.\n",
    "\n",
    "Example:[ Your previous choice was A. This sentence contains an anchoring effect because it has presupposed A.]\n",
    "[ Studies have shown that drinking coffee can improve alertness and concentration. There is no cognitive bias in this sentence. ]\n",
    "\n",
    "Prompt:[ %s ]\n",
    "'''%(input))\n",
    "    #print(ans)\n",
    "    now=ans.split('[')\n",
    "    #print(now)\n",
    "    for i in now:\n",
    "        if len(i)==0:\n",
    "            continue\n",
    "        if \"not contain\" in i or \"Not contain\" in i or 'no cognitive bias' in i or 'No cognitive bias' in i:\n",
    "            a=1\n",
    "        elif 'contain' in i:\n",
    "            return 1\n",
    "        if 'contains' in i:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def judge_com(input):\n",
    "    ans=get_output('''Please help me determine whether the following question is complete or not.\n",
    "\n",
    "Prompt:[%s]\n",
    "'''%(input))\n",
    "    #print(ans)\n",
    "    if \"Incomplete\" in ans or \"incomplete\" in ans or 'not' in ans:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [22:31<00:00,  2.70s/it]\n",
      " 80%|███████▉  | 398/500 [07:38<01:57,  1.15s/it]\n"
     ]
    },
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7684\\1093662401.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m#print(prompt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m#print(try_num)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mans\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\nA.no     B.yes\\n\\nOutput format:\\nAns: (Only one option can be selected)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"without_think.jsonl\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0minput_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'number'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ans'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ans'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'output'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mans\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'prompt'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7684\\382299686.py\u001b[0m in \u001b[0;36mget_output\u001b[1;34m(message)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mapi_key\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey_pool\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlinshi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     )\n\u001b[1;32m----> 6\u001b[1;33m     chat_completion = client.chat.completions.create(\n\u001b[0m\u001b[0;32m      7\u001b[0m         messages=[\n\u001b[0;32m      8\u001b[0m             {\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\groq\\resources\\chat\\completions.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    287\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \"\"\"\n\u001b[1;32m--> 289\u001b[1;33m         return self._post(\n\u001b[0m\u001b[0;32m    290\u001b[0m             \u001b[1;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             body=maybe_transform(\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\groq\\_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1223\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"post\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1224\u001b[0m         )\n\u001b[1;32m-> 1225\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1227\u001b[0m     def patch(\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\groq\\_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    918\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[1;32m--> 920\u001b[1;33m         return self._request(\n\u001b[0m\u001b[0;32m    921\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m             \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\groq\\_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m             \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Re-raising status error\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1018\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1019\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m         return self._process_response(\n",
      "\u001b[1;31mInternalServerError\u001b[0m: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}"
     ]
    }
   ],
   "source": [
    "data= open('../data/test.jsonl','r')\n",
    "for i in trange(500):\n",
    "    question=json.loads(data.readline())\n",
    "    all_num=0\n",
    "    linshi=get_prompt(str(question['ans']),question['question'])\n",
    "    while all_num<=0:\n",
    "        prompt=linshi\n",
    "        try_num=0\n",
    "        while judge_bias(prompt) and try_num<=2:\n",
    "            try:\n",
    "                #judge=re.findall(r\"Recognize Bias:([\\S|\\s]*)\",judge, re.S|re.I)[0]\n",
    "                #print(judge)\n",
    "                prompt=get_output(rewrite_bias(prompt))\n",
    "                #prompt=re.findall(r\"Unbiased Prompt:([\\S|\\s]*)\",temp, re.S|re.I)[0]\n",
    "                #print(prompt)\n",
    "                try:\n",
    "                    prompt=re.findall(r\"Task:([\\S|\\s]*)\",prompt, re.S|re.I)[0]\n",
    "                    try:\n",
    "                        prompt=re.findall(r\"([\\S|\\s]*)\\?([\\S|\\s]*)\",prompt, re.S|re.I)[0][0]\n",
    "                        prompt=prompt+'?'\n",
    "                    except:\n",
    "                        a=1\n",
    "                except:\n",
    "                    a=1\n",
    "                try_num=try_num+1\n",
    "            except:\n",
    "                continue\n",
    "        all_num=all_num+1\n",
    "        if judge_com(prompt):\n",
    "            break\n",
    "    try:\n",
    "        prompt=re.findall(r\"Task:([\\S|\\s]*)\",prompt, re.S|re.I)[0]\n",
    "        try:\n",
    "            prompt=re.findall(r\"([\\S|\\s]*)\\?([\\S|\\s]*)\",prompt, re.S|re.I)[0][0]\n",
    "            prompt=prompt+'?'\n",
    "        except:\n",
    "            a=1\n",
    "    except:\n",
    "        a=1\n",
    "    prompt=prompt.replace('[','').replace(']','').replace('*','').strip()\n",
    "    #print(prompt)\n",
    "    #print(try_num)\n",
    "    ans=get_output(prompt+'\\nA.no     B.yes\\n\\nOutput format:\\nAns: (Only one option can be selected)')\n",
    "    with open(\"without_think.jsonl\",\"a\",encoding='utf-8') as k:\n",
    "        input_dict={'number':i,'ans':str(question['ans']),'output':ans,'prompt':prompt}\n",
    "        input_json=json.dumps(input_dict)\n",
    "        k.write(input_json+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
