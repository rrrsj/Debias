{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from random import randint\n",
    "from tqdm import trange\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from groq import Groq\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_output(message):\n",
    "    linshi=random.randint(0,4)\n",
    "    client = Groq(\n",
    "        api_key=key_pool[linshi],\n",
    "    )\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": message,\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-70b-8192\",\n",
    "        temperature=0\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_prompt(fact):\n",
    "    template='''Please answer the question.\n",
    "\n",
    "Question: %s\n",
    "'''%(fact)\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def recognize_bias(bias):\n",
    "    template='''The following is a medical task prompt may contain cognitive biases. Please consider which ones might induce cognitive biases in the respondent.\n",
    "\n",
    "Example:[ Your previous choice was A. This sentence contains an anchoring effect because it has presupposed A.\n",
    "Studies have shown that drinking coffee can improve alertness and concentration. There is no cognitive bias in this sentence. ]  \n",
    "\n",
    "Prompt: [%s]\n",
    "\n",
    "Output Format:\n",
    "Bias sentence1:\n",
    "Bias sentence2:\n",
    "...\n",
    "'''%(bias)\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def rewrite_bias(bias,judge):\n",
    "    template='''The following is a medical task prompt may contain cognitive biases. Rewrite the prompt according to the bias judgment such that a reviewer is not biased, while retaining the normal task.\n",
    "Prompt: [%s]\n",
    "\n",
    "Bias Judgment:[%s]\n",
    "\n",
    "Output Format: \n",
    "Unbiased Prompt:\n",
    "\n",
    "'''%(bias,judge)\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def rewrite_bias(bias,judge):\n",
    "    template='''The following is a medical choice task may contain cognitive biases. Rewrite the task in [] according to the bias judgment such that a reviewer is not biased, while retaining the normal sentences.\n",
    "\n",
    "Bias Judgment:[%s]\n",
    "\n",
    "Task: [%s]\n",
    "\n",
    "Here is new task without bias:\n",
    "Task:'''%(judge,bias)\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def judge_bias(input):\n",
    "    ans=get_output('''Please break prompt all in [] into sentence by sentence, and determine whether may contain cognitive biases that affect normal selection, and provide the reasons for the cognitive biases.\n",
    "\n",
    "Example:[ Your previous choice was A. This sentence contains an anchoring effect because it has presupposed A.]\n",
    "[ Studies have shown that drinking coffee can improve alertness and concentration. There is no cognitive bias in this sentence. ]\n",
    "\n",
    "Prompt:[ %s ]\n",
    "'''%(input))\n",
    "    #print(ans)\n",
    "    now=ans.split('[')\n",
    "    #print(now)\n",
    "    for i in now:\n",
    "        if len(i)==0:\n",
    "            continue\n",
    "        if \"not contain\" in i or \"Not contain\" in i or 'no cognitive bias' in i or 'No cognitive bias' in i:\n",
    "            a=1\n",
    "        elif 'contain' in i:\n",
    "            return 1\n",
    "        if 'contains' in i:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def judge_com(input):\n",
    "    ans=get_output('''Please help me determine whether the following question is complete or not.\n",
    "\n",
    "Prompt:[%s]\n",
    "\n",
    "'''%(input))\n",
    "    #print(ans)\n",
    "    if \"Incomplete\" in ans or \"incomplete\" in ans or 'not' in ans:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [15:16<00:00,  1.83s/it]\n",
      " 69%|██████▉   | 345/500 [01:37<00:43,  3.53it/s]\n"
     ]
    },
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4768\\1785069170.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mall_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_num\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mjudge_com\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4768\\176610738.py\u001b[0m in \u001b[0;36mjudge_com\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mjudge_com\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     ans=get_output('''Please help me determine whether the following question is complete or not.\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mPrompt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4768\\382299686.py\u001b[0m in \u001b[0;36mget_output\u001b[1;34m(message)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mapi_key\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey_pool\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlinshi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     )\n\u001b[1;32m----> 6\u001b[1;33m     chat_completion = client.chat.completions.create(\n\u001b[0m\u001b[0;32m      7\u001b[0m         messages=[\n\u001b[0;32m      8\u001b[0m             {\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\groq\\resources\\chat\\completions.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    287\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \"\"\"\n\u001b[1;32m--> 289\u001b[1;33m         return self._post(\n\u001b[0m\u001b[0;32m    290\u001b[0m             \u001b[1;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             body=maybe_transform(\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\groq\\_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1223\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"post\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1224\u001b[0m         )\n\u001b[1;32m-> 1225\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1227\u001b[0m     def patch(\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\groq\\_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    918\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[1;32m--> 920\u001b[1;33m         return self._request(\n\u001b[0m\u001b[0;32m    921\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m             \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\groq\\_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m             \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Re-raising status error\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1018\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1019\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m         return self._process_response(\n",
      "\u001b[1;31mInternalServerError\u001b[0m: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}"
     ]
    }
   ],
   "source": [
    "data= open('../data/test.jsonl','r')\n",
    "for i in trange(500):\n",
    "    question=json.loads(data.readline())\n",
    "    if i<=326:\n",
    "        continue\n",
    "    all_num=0\n",
    "    linshi=get_prompt(question['question'])\n",
    "    while all_num<=0:\n",
    "        prompt=linshi\n",
    "        judge=get_output(recognize_bias(prompt))\n",
    "        #judge=re.findall(r\"Recognize Bias:([\\S|\\s]*)\",judge, re.S|re.I)[0]\n",
    "        #print(judge)\n",
    "        prompt=get_output(rewrite_bias(prompt,judge))\n",
    "        #prompt=re.findall(r\"Unbiased Prompt:([\\S|\\s]*)\",temp, re.S|re.I)[0]\n",
    "        #print(prompt)\n",
    "        try:\n",
    "            prompt=re.findall(r\"Task:([\\S|\\s]*)\",prompt, re.S|re.I)[0]\n",
    "        except:\n",
    "            a=1\n",
    "        all_num=all_num+1\n",
    "        if judge_com(prompt):\n",
    "            break\n",
    "    try:\n",
    "        prompt=re.findall(r\"Task:([\\S|\\s]*)\",prompt, re.S|re.I)[0]\n",
    "    except:\n",
    "        a=1\n",
    "    prompt=prompt.replace('[','').replace(']','').replace('*','').strip()\n",
    "    #print(prompt)\n",
    "    #print(try_num)\n",
    "    ans=get_output(prompt+'\\nA.no     B.yes\\n\\nOutput format:\\nAns:  (Only one option can be selected)')\n",
    "    with open(\"without_while.jsonl\",\"a\",encoding='utf-8') as k:\n",
    "        input_dict={'number':i,'ans':str(question['ans']),'output':ans,'prompt':prompt}\n",
    "        input_json=json.dumps(input_dict)\n",
    "        k.write(input_json+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
